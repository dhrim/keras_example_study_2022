{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongeunKim-sonja/keras_example_study_2022/blob/main/Automatic_Speech_Recognition_using_CTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CTCë¥¼ ì´ìš©í•œ ìŒì„± ìë™ ì¸ì‹ ì˜ˆì œ\n",
        "\n",
        "*ì´ ë¬¸ì„œëŠ” keras ì˜ˆì œë¥¼ ë¶„ì„í•˜ì—¬ í•´ì„¤ì„ ì‘ì„±í•œ ê²ƒì…ë‹ˆë‹¤.*\n",
        "\n",
        "<br>[ì˜ˆì œ í˜ì´ì§€](https://keras.io/examples/audio/ctc_asr/)\n",
        "<br>[CTC ì°¸ê³ ](https://ratsgo.github.io/speechbook/docs/neuralam/ctc)\n",
        "<br>[ìŒì„±ì¸ì‹ ë”¥ëŸ¬ë‹ ê°•ì˜ ì˜ìƒ](https://youtu.be/MghbHLuupoA)\n",
        "<br>[ê°•ì˜ ì˜ìƒ ì •ë¦¬ ê¸€](https://lynnshin.tistory.com/42)\n",
        "\n",
        "<br>ì´ í˜ì´ì§€ì˜ ì˜ˆì œ ì½”ë“œëŠ” 2D CNNê³¼ RNN, CTC lossë¥¼ ì´ìš©í•˜ì—¬ ìŒì„± ìë™ì¸ì‹(ASR)ì„ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤. \n",
        "<br>ë°ì´í„° ì…‹ì€ LibriVox í”„ë¡œì íŠ¸ì˜ LJSSpeech ë°ì´í„°ì…‹(ë…¼í”½ì…˜ ì±… 7ê¶Œì˜ ë¬¸ì¥ì„ ë…¹ìŒí•œ ì˜¤ë””ì˜¤ í´ë¦½)ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "<br>ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ íŒë³„í•˜ê¸° ìœ„í•´ì„œëŠ” WER(ë‹¨ì–´ ì˜¤ë¥˜ìœ¨)ì„ ì‚¬ìš©í•˜ëŠ”ë°, ì˜¤ë¥˜ìœ¨ì˜ ê³„ì‚°ì„ ìœ„í•´ jiweríŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "knFAq6rtEgvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CTC\n",
        "input ë°ì´í„°ì™€ output ë°ì´í„° ì‚¬ì´ì˜ êµ¬ê°„ë³„ 1ëŒ€ 1 ë¼ë²¨ë§ì´ ë˜ì–´ìˆì§€ ì•Šì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë¶„ë¥˜ ê¸°ë²•.<br>\n",
        "CTCëª¨ë¸ì˜ ê¸°ë³¸ì€ RNNê¸°ë°˜ ì‹œí€€ìŠ¤ëª¨ë¸ ì—¬ëŸ¬ê°œì™€ softmaxë¥¼ í†µê³¼í•´ì„œ ì¶œë ¥í•¨.<br>\n",
        "ìŒì†Œì‚¬ì´ì—ëŠ” ë¬µìŒì„ ì²˜ë¦¬í•˜ëŠ” ì…ì‹¤ë¡ Îµì´ ì¶”ê°€ë¨.<br><br>\n",
        "\n",
        "1. êµ¬ê°„ì„ ë‚˜ëˆ„ì–´ ìŒì†Œë¥¼ íŒë³„\n",
        "![CTC1](https://i.imgur.com/hpVlJXr.png){: width=\"30\" height=\"30\"}\n",
        "<br><br>\n",
        "2. ì…ì‹¤ë¡ (ë¬µìŒ)êµ¬ê°„ ë° ë°˜ë³µë˜ëŠ” ìŒì†Œ ì‚­ì œ\n",
        "![CTC2](https://i.imgur.com/LRjrS68.png){: width=\"50\" height=\"50\"}\n"
      ],
      "metadata": {
        "id": "Uzmj2ILHJHmf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "695fvAJSEVlY"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kmic9K1EhtH",
        "outputId": "2e9e4e92-bc92-45ab-daff-2badb9a47288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
            "Collecting levenshtein==0.20.2\n",
            "  Downloading Levenshtein-0.20.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.2 MB 56.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein, jiwer\n",
            "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTw5gqhQEVlY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from jiwer import wer   #ë‹¨ì–´ ì˜¤ë¥˜ìœ¨ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsyweKDuEVla"
      },
      "source": [
        "## Load the LJSpeech Dataset\n",
        "\n",
        "Let's download the [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/).\n",
        "The dataset contains 13,100 audio files as `wav` files in the `/wavs/` folder.\n",
        "The label (transcript) for each audio file is a string\n",
        "given in the `metadata.csv` file. The fields are:\n",
        "\n",
        "- **ID**: this is the name of the corresponding .wav file\n",
        "- **Transcription**: words spoken by the reader (UTF-8)\n",
        "- **Normalized transcription**: transcription with numbers,\n",
        "ordinals, and monetary units expanded into full words (UTF-8).\n",
        "\n",
        "For this demo we will use on the \"Normalized transcription\" field.\n",
        "\n",
        "Each audio file is a single-channel 16-bit PCM WAV with a sample rate of 22,050 Hz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DovrgQz3EVla",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "f2bb05b8-8c61-44e7-fe03-fe591b33117f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "2748572632/2748572632 [==============================] - 28s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    file_name                           normalized_transcription\n",
              "0  LJ036-0013  found along the path of flight taken by the gu...\n",
              "1  LJ040-0227  Mrs. Siegel concluded her report with the stat...\n",
              "2  LJ014-0323  The warrant thus represented money, and was of..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb34681b-c7f3-4ecf-b4a1-50e691e2f17e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>normalized_transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LJ036-0013</td>\n",
              "      <td>found along the path of flight taken by the gu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LJ040-0227</td>\n",
              "      <td>Mrs. Siegel concluded her report with the stat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LJ014-0323</td>\n",
              "      <td>The warrant thus represented money, and was of...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb34681b-c7f3-4ecf-b4a1-50e691e2f17e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bb34681b-c7f3-4ecf-b4a1-50e691e2f17e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bb34681b-c7f3-4ecf-b4a1-50e691e2f17e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
        "data_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar=True)\n",
        "wavs_path = data_path + \"/wavs/\"\n",
        "metadata_path = data_path + \"/metadata.csv\"\n",
        "\n",
        "\n",
        "# Read metadata file and parse it\n",
        "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\n",
        "metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
        "metadata_df = metadata_df[[\"file_name\",\"normalized_transcription\"]]\n",
        "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
        "metadata_df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g7MFhcFEVlb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c39bda7-35c8-4c3c-cf26-f685a6612b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the training set: 11790\n",
            "Size of the validation set: 1310\n"
          ]
        }
      ],
      "source": [
        "split = int(len(metadata_df) * 0.90)    #ë°ì´í„°ì…‹ì„ 9:1ì˜ ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì–´ ê°ê° í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ìš©í•¨\n",
        "df_train = metadata_df[:split]\n",
        "df_val = metadata_df[split:]\n",
        "\n",
        "print(f\"Size of the training set: {len(df_train)}\")\n",
        "print(f\"Size of the validation set: {len(df_val)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOzcZqCKEVlc"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "We first prepare the vocabulary to be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRkavm2EVlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd226077-658a-429e-f3f8-0e60d67d22f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\", '?', '!', ' '] (size =31)\n"
          ]
        }
      ],
      "source": [
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]    #ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ list comprehension, ì°¸ê³  https://wikidocs.net/22805\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")    \n",
        "#oov_token ì„¤ì •í•œ ê²½ìš°ì—ëŠ” ê·¸ ê°’ì„ indexì— ì¶”ê°€í•˜ì—¬ out of vocabulary ì¸ìë¥¼ ëŒ€ì²´í•œë‹¤. (ì˜ˆì™¸ì²˜ë¦¬)\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJohkL8lEVld"
      },
      "source": [
        "Next, we create the function that describes the transformation that we apply to each\n",
        "element of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCU2bcToEVld"
      },
      "outputs": [],
      "source": [
        "# An integer scalar Tensor. The window length in samples.\n",
        "frame_length = 256\n",
        "# An integer scalar Tensor. The number of samples to step.\n",
        "frame_step = 160\n",
        "# An integer scalar Tensor. The size of the FFT to apply.\n",
        "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
        "#ì œì‹œë˜ì§€ ì•Šì€ ê²½ìš° frame_lengthë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€, ê°€ì¥ ì‘ì€ 2ì˜ ì œê³±ìˆ˜ë¡œ í•œë‹¤.\n",
        "fft_length = 384\n",
        "\n",
        "\n",
        "def encode_single_sample(wav_file, label):\n",
        "    #ìƒ˜í”Œì„ ì¸ì½”ë”© í•˜ëŠ” ê³¼ì •, ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ì–»ì€ ë’¤ ì§„í­ê°’(ì ˆëŒ€ê°’)ë§Œì„ ì·¨í•˜ê³ , í‘œì¤€í™” í•˜ëŠ” ìˆœì„œë¡œ ì§„í–‰ëœë‹¤. \n",
        "    #ì˜ì–´ëŠ” ëŒ€,ì†Œë¬¸ìê°€ ë‚˜ë‰˜ì–´ ìˆìœ¼ë¯€ë¡œ, ëª¨ë‘ ì†Œë¬¸ìë¡œ í†µì¼í•˜ì—¬ case ê°’ì„ ì¤„ì¸ë‹¤. \n",
        "    ###########################################\n",
        "    ##  Process the Audio\n",
        "    ##########################################\n",
        "    # 1. Read wav file\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    # 2. Decode the wav file\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)  \n",
        "    # 3. Change type to float\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # 4. Get the spectrogram\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    # 6. normalisation\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "    ###########################################\n",
        "    ##  Process the label\n",
        "    ##########################################\n",
        "    # 7. Convert label to Lower case\n",
        "    label = tf.strings.lower(label)\n",
        "    # 8. Split the label\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    # 9. Map the characters in label to numbers\n",
        "    label = char_to_num(label)\n",
        "    # 10. Return a dict as our model is expecting two inputs\n",
        "    return spectrogram, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh4av06BEVld"
      },
      "source": [
        "## Creating `Dataset` objects\n",
        "\n",
        "We create a `tf.data.Dataset` object that yields\n",
        "the transformed elements, in the same order as they\n",
        "appeared in the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHf6pS6nEVle"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "# Define the trainig dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n",
        ")\n",
        "train_dataset = (\n",
        "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Define the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n",
        ")\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGfETZ66EVle"
      },
      "source": [
        "## Visualize the data\n",
        "\n",
        "Let's visualize an example in our dataset, including the\n",
        "audio clip, the spectrogram and the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0RQ_I-dEVle"
      },
      "outputs": [],
      "source": [
        "#ë°ì´í„° ì‹œê°í™”\n",
        "#ìŠ¤í™íŠ¸ë¡œê·¸ë¨ê³¼ ë¼ë²¨, ì§„í­ì„ ê·¸ë¦° ëª¨ì–‘ì„ ì¶œë ¥í•œë‹¤.\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "for batch in train_dataset.take(1):\n",
        "    spectrogram = batch[0][0].numpy()\n",
        "    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
        "    label = batch[1][0]\n",
        "    # Spectrogram\n",
        "    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "    ax = plt.subplot(2, 1, 1)\n",
        "    ax.imshow(spectrogram, vmax=1)\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "    # Wav\n",
        "    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = audio.numpy()\n",
        "    ax = plt.subplot(2, 1, 2)\n",
        "    plt.plot(audio)\n",
        "    ax.set_title(\"Signal Wave\")\n",
        "    ax.set_xlim(0, len(audio))\n",
        "    display.display(display.Audio(np.transpose(audio), rate=16000))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1IRZTEmEVlf"
      },
      "source": [
        "## Model\n",
        "\n",
        "We first define the CTC Loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiLGpjDcEVlf"
      },
      "outputs": [],
      "source": [
        "#Lossí•¨ìˆ˜ë¡œ ì‚¬ìš©ë  CTCLoss ë¥¼ êµ¬í˜„í•œë‹¤. \n",
        "def CTCLoss(y_true, y_pred):\n",
        "    # Compute the training-time loss value\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eDwsI93EVlf"
      },
      "source": [
        "We now define our model. We will define a model similar to\n",
        "[DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3vxUlZEEVlf"
      },
      "outputs": [],
      "source": [
        "#ëª¨ë¸ì˜ ë ˆì´ì–´ë¥¼ êµ¬ì„±í•œë‹¤. \n",
        "#Baidu ì—ì„œ ë°œí‘œí•œ DeepSpeech2(2015)ì˜ ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ê²Œ êµ¬ì„±í•˜ì˜€ë‹¤.\n",
        "#Conv 2íšŒ ë°˜ë³µ í›„ RNNë ˆì´ì–´ë¥¼ ë°˜ë³µë¬¸ìœ¼ë¡œ êµ¬ì„±í•œë‹¤. ì´ ëª¨ë¸ì—ì„œëŠ” 5ê°œì˜ RNNë ˆì´ì–´ê°€ ì‚¬ìš©ë˜ì—ˆë‹¤. \n",
        "#OptimizerëŠ” Adam, Loss functionì€ ìœ„ì—ì„œ ì„ ì–¸í•œ CTCLoss í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. ì´ ë•ë¶„ì— í•´ë‹¹ ëª¨ë¸ì—ì„œëŠ” ë””ì½”ë”ë¥¼ ë³„ë„ë¡œ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ”ë‹¤. \n",
        "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "    # Model's input\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "    # Convolution layer 1\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 41],\n",
        "        strides=[2, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_1\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
        "    # Convolution layer 2\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 21],\n",
        "        strides=[1, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_2\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
        "    # Reshape the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "    # RNN layers\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "        )\n",
        "        x = layers.Bidirectional(\n",
        "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "        )(x)\n",
        "        if i < rnn_layers:\n",
        "            x = layers.Dropout(rate=0.5)(x)\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "    x = layers.Dropout(rate=0.5)(x)\n",
        "    # Classification layer\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    # Model\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = build_model(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size(),\n",
        "    rnn_units=512,\n",
        ")\n",
        "model.summary(line_length=110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSrlqdWaEVlg"
      },
      "source": [
        "## Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSKRfhMpEVlg"
      },
      "outputs": [],
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    # greedy searchëŠ” ê·¸ë•Œê·¸ë•Œ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ê¸° ë•Œë¬¸ì— ì—°ì‚° ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ ë¬¸ë²•ì ìœ¼ë¡œ ì´ìƒí•´ì§€ëŠ” ê²½ìš°ê°€ ìˆìŒ\n",
        "    # beam searchëŠ” ë¯¸ë˜ë¥¼ ê³ ë ¤í•˜ì—¬ í™•ë¥ ì´ ê°–ì¥ ë†’ì€ ì¡°í•© ì„ íƒ, ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ í•„ìˆ˜ì ì´ë‚˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ê³„ì‚°ì–‘ì´ ë†’ë‹¤.\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (\n",
        "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "                targets.append(label)\n",
        "        wer_score = wer(targets, predictions)\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "        print(\"-\" * 100)\n",
        "        for i in np.random.randint(0, len(predictions), 2):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "            print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p_lwH0vEVlg"
      },
      "source": [
        "Let's start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uAQsV4gEVlg"
      },
      "outputs": [],
      "source": [
        "# Define the number of epochs. ì—í­ ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤.\n",
        "epochs = 1\n",
        "# Callback function to check transcription on the val set.\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[validation_callback],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcaMFIntEVlh"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ_1ErynEVlh"
      },
      "outputs": [],
      "source": [
        "# Let's check results on more validation samples\n",
        "#ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤. ê° Epoch ë§ˆë‹¤ ë‹¨ì–´ì˜¤ë¥˜ìœ¨WERì„ í‘œê¸°í•˜ê³ , ì›ë˜ì˜ ë¬¸ì¥ê³¼ ì˜ˆì¸¡í•œ ë¬¸ì¥ì„ ì°¨ë¡€ë¡œ ì¶œë ¥í•œë‹¤. \n",
        "predictions = []\n",
        "targets = []\n",
        "for batch in validation_dataset:\n",
        "    X, y = batch\n",
        "    batch_predictions = model.predict(X)\n",
        "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "    predictions.extend(batch_predictions)\n",
        "    for label in y:\n",
        "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "        targets.append(label)\n",
        "wer_score = wer(targets, predictions)\n",
        "print(\"-\" * 100)\n",
        "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "print(\"-\" * 100)\n",
        "for i in np.random.randint(0, len(predictions), 5):\n",
        "    print(f\"Target    : {targets[i]}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrrePEX9EVlh"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In practice, you should train for around 50 epochs or more. Each epoch\n",
        "takes approximately 5-6mn using a `GeForce RTX 2080 Ti` GPU.\n",
        "The model we trained at 50 epochs has a `Word Error Rate (WER) â‰ˆ 16% to 17%`.\n",
        "\n",
        "í•´ë‹¹ ë¬¸ì„œì—ì„œëŠ” 50 epochì§„í–‰ ê²°ê³¼ WER ê°’ì´ 16~17%ì •ë„ë¡œ ë‚®ì•„ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤. \n",
        "ì•„ë˜ëŠ” 50 epochì˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•œ ê²ƒì´ë‹¤. \n",
        "\n",
        "Some of the transcriptions around epoch 50:\n",
        "\n",
        "**Audio file: LJ017-0009.wav**\n",
        "```\n",
        "- Target    : sir thomas overbury was undoubtedly poisoned by lord rochester in the reign\n",
        "of james the first\n",
        "- Prediction: cer thomas overbery was undoubtedly poisoned by lordrochester in the reign\n",
        "of james the first\n",
        "```\n",
        "\n",
        "**Audio file: LJ003-0340.wav**\n",
        "```\n",
        "- Target    : the committee does not seem to have yet understood that newgate could be\n",
        "only and properly replaced\n",
        "- Prediction: the committee does not seem to have yet understood that newgate could be\n",
        "only and proberly replace\n",
        "```\n",
        "\n",
        "**Audio file: LJ011-0136.wav**\n",
        "```\n",
        "- Target    : still no sentence of death was carried out for the offense and in eighteen\n",
        "thirtytwo\n",
        "- Prediction: still no sentence of death was carried out for the offense and in eighteen\n",
        "thirtytwo\n",
        "```\n",
        "\n",
        "Example available on HuggingFace.\n",
        "| Trained Model | Demo |\n",
        "| :--: | :--: |\n",
        "| [![Generic badge](https://img.shields.io/badge/ğŸ¤—%20Model-CTC%20ASR-black.svg)](https://huggingface.co/keras-io/ctc_asr) | [![Generic badge](https://img.shields.io/badge/ğŸ¤—%20Spaces-CTC%20ASR-black.svg)](https://huggingface.co/spaces/keras-io/ctc_asr) |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#í™•ì¥\n",
        "\n",
        "- í•œêµ­ì–´ ìŒì„±ì˜ ê²½ìš°ì—ëŠ” ì–´ë–¤ ì‹ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ì¢‹ì„ê¹Œ?<br>\n",
        "  github sooftwareì˜ kospeech ìœ„í‚¤ ë¬¸ì„œì— ë”°ë¥´ë©´, ê¸°ì¡´ì— ì˜ˆìƒí–ˆë˜ ëŒ€ë¡œ ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ìŒì†Œ(2337ê°œ)ë¥¼ í•™ìŠµí•œ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.<br> [github wiki ë¬¸ì„œ](https://github.com/sooftware/KoSpeech/wiki/Preparation-before-Training)\n",
        "  <br>\n",
        "  'ì¹ ì‹­ í¼ì„¼íŠ¸' ë¬¸ì¥ì˜ ê²½ìš° \n",
        "  '318 119 0 489 551 156' ì™€ ê°™ì´ ì¶œë ¥ëœë‹¤. \n",
        "  <br>\n",
        "  ì´ë•Œ 0ì€ ê³µë°±ë¬¸ìì´ë‹¤.<br>\n",
        "  ë‚®ì€ ë‹¨ì–´ì˜¤ë¥˜ìœ¨ì„ ìœ„í•´ ì¶©ë¶„íˆ ë§ì€ ë°ì´í„°ì…‹ì„ í•„ìš”ë¡œ í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\n",
        "  <br>\n",
        "  ì´ì™¸ì—ë„ Hybrid CTC-Attentionì„ ì´ìš©í•œ ìŒì„± ì¸ì‹ ëª¨ë¸ì˜ ê²½ìš° ê°ê° ììŒê³¼ ëª¨ìŒì„ ìŒì†Œë¡œ ë¶„ë¦¬í•˜ì—¬ 49ê°œ ìì†Œë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•˜ì˜€ë‹¤.\n",
        "\n",
        "  [ì°¸ê³  ë…¼ë¬¸ ë§í¬](https://koreascience.kr/article/CFKO201832073078862.pdf)\n",
        "  <br><br>\n",
        "- ê¸°ì¡´ì— ë°œí‘œëœ Deep speech2ëŠ” Baiduì—ì„œ ë°œí‘œí•˜ì—¬ ì¤‘êµ­ì–´ ìŒì„±ì„ ì¸ì‹í•˜ëŠ” ëª¨ë¸ì¸ ê²ƒ ê°™ì€ë°, ì¤‘êµ­ì–´ëŠ” ë°œìŒê¸°í˜¸(ë³‘ìŒ) í‘œê¸°ê°€ ë¡œë§ˆìë¡œ ì´ë£¨ì–´ì ¸ ë¹„êµì  ìˆ˜ì›”í•´ë³´ì¸ë‹¤. <br>ë‹¤ë§Œ ë™ì¼í•œ ë³‘ìŒì´ì–´ë„ ì„±ì¡°ê°€ ë‹¤ë¥¸ ê²½ìš° ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ ì¶”í›„ í™•ì¸í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. <br> -> Deep speech2ì˜ ê²½ìš° ì¤‘êµ­ì–´ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì—¬ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” 6,000ê°œì˜ ë¬¸ìë¥¼ ì¶œë ¥ìœ¼ë¡œ ì •ì˜í•˜ì˜€ë‹¤ê³  í•œë‹¤.<br>\n",
        "[Deep speech2 ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/1512.02595)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O8uNTDKb1_gZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}